---
title: "Multifactor Productivity Forecasting"
author: "Steph de Silva"
date: "03/12/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The annual release of the Australian national productivity statistics is only days away - this is an area I've worked in for [quite some time](http://rex-analytics.com/productivity-long-run-nearly-everything/). I'm interested to see what's been happening over the last year. As it happens, I've also been reading through [Hyndman and Athanasopolous](http://otexts.org/fpp2/) brushing up on a few things I haven't looked at in a decade or more (!) so I thought this was the perfect time for a a basic forecast to tide me over until release. The code comes straight from their book.

Here, I'll be making extensive use of the [forecast package](https://CRAN.R-project.org/package=forecast) by Hyndman and coauthors. It's an incredibly useful piece of work that provides a helpful structure if time series nuances are not your thing, as well as a lot of flexibility if they are.

The data I'll be using is the 2015-16 of [Estimates of Industry Multifactor Productivity, Australia (series 5260.0.55.002)](http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/5260.0.55.0022015-16?OpenDocument) which was released on December 5, 2016. It's December 4, 2017 at the time of writing: let's see if we can come up with a forecast for Multifactor productivity for this past year.

# Step 1: Load the data, set up a useful working environment for a basic forecast.

The ABS has a tendency to provide its data in all sorts of weird formats - or at least it seems weird to people who don't use Excel alot.

![It's not messy, but it's not "tidy"](images/not_messy.png)

In order to get started I needed to do a fair bit of cleaning up. In brief, there were a few steps:

    * Load the data, turn it sideways so the variables are in columns
    * Replace the column names with something more workable
    * Get rid of empty/not useful variables
    * Replace the ABS na error code with NA for R compatibility
    * Reindex to 2013-14 financial year, more on that in a second
    * Convert to a ts object
    
The last two steps are worth talking about: productivity is measured as an _index_, so there has to be a base year somewhere in the series that all other years are compared to. I tend to use a base year of 2013-14 as that's what my coauthors use.

I also converted the dataframe to a ts object. This is a really useful datatype that comes with all sorts of built in tools via the forecast package.

```{r setupMFP, fig.show = 'asis', include=TRUE}
library(tidyverse)
library(readxl)
library(forecast)


productivity<- read_xls("data/52600550022016.xls", sheet="Table 4", 
                     col_names = FALSE, skip=10, n_max = 17)

# the data is sideways and has difficult variable names - let's fix that!
productivity<-t(productivity) %>%
                as.data.frame()
colnames(productivity)<-c("LabourHW", 
                      "LabourQA", 
                      "Capital", 
                      "Mutlifactor.Productivity.Null" ,
                      "MultifactorHW", 
                      "MultifactorQA")

# Getting rid of empty space and unnecessary bits
productivity <- productivity[-1,]
productivity <- productivity[,c(1:3,5:6)]


# A few things going on here:
# 1. Replacing the ABS "na" with NA for R compatibility
# 2. Read the data in as numeric
# 3. Convert to data frame
# 4. Reindex to 2013-14 year
# 5. Convert to a time series object
productivity <- productivity %>% 
                    map(function(x) replace(x, x == "na", NA)) %>%
                    map(function(x) x = as.numeric(as.character(x))) %>%
                    as.data.frame()%>%
                    sapply(function(vec){return(vec/vec[31])*100}, 
                           simplify=TRUE) %>%
                    ts(start = 1973)
# Reindex to 2003-2004 which is row 31 in data frame

```


That was a fair bit of data wrangling, let's take a look at what we're given with the `autoplot()` function from `forecast`. This function is a quick way to visualise your data when you're working through it interactively. It works with all the usual `ggplot2` 'bits' and I've added `theme_light()` here because I'm not into grey rectangles.

Here I'm just looking at three productivity series: labour productivity (hours worked), capital productivity and multifactor productivity (hours worked), which is a combination of the two. The series dates back to 1973-4 and ends at 2015-16.

```{r autoplot, fig.show = 'as.is', include=TRUE}
autoplot(productivity[,c(1,3,4)])+
  theme_light()
```

When looking at time series data, the next thing I like to do is look at autocorrelation and partial autocorrelation functions. These give me a good feel for what the structure of the series might be. `Forecast` gives the `ggAcf()` function and the `ggPacf()` function. Both of which rocked my world when I found out about them. When I first started using ACF and PACFs, they came out as combinations of x and - printed straight into a console. They were ugly as hell. The ggplot treatment goes to show how far we've come in 15 years.

Here's the autocorrelation function (ACF): notice the blue lines? Those are the significance bounds. Autocorrelations outside those are very probably different from zero (e.g. not chance). 

```{r acf1, fig.show = 'asis', include=TRUE}
ggAcf(productivity[,4]) + 
  ggtitle("Multifactor productivity ACF")+
  theme_light()
```

Next up the partial autocorrelation function (PACF). One 'blip' in the first lag that is significant and nothing much after. Combined with a long slow decay in the ACF - this means high levels of a first order autoregression in most cases, but not much in the way of moving average components, most likely.

```{r pacf1, fig.show = 'asis', include=TRUE}
ggPacf(productivity[,4]) + 
  ggtitle("Multifactor productivity PACF")+
  theme_light()
```

# A simple forecast

The `forecast` package provides some very simple tools for producing forecasts. What we can see from our plots above, though, is pretty consistent with common sense:
    
    * Both labour and capital productivity contribute to multifactor productivity: in fact, that's how you get multifactor productivity!
    * There appears to be some kind of trend in there, but it almost certainly isn't linear over the whole series. There are three pieces in my view - from 1973 to about 1987, 1987 - 2003 and then 2014 onwards. This is roughly consistent with the literature, particularly the post-2003 slowdown.
    * There are significant amounts of autoregressive behaviour in the series and the series may be non-stationary (changing over time).
    
How can we tell if we're dealing with a nonstationary time series? The package `tseries` has some useful functions. The `adf.test()` function uses the Augmented Dickey Fuller (Said and Dickey 1984) test for a null hypothesis of a unit root, while the `kpss.test()` function tests for a null hypothesis of stationarity (Kwiatkowski, Phillips, Schmidt and Shin 1992). If these two tests produce different outcomes (e.g. one rejects the null, the other does not) - they're in agreement. In practice, these tests tend to come with lower power as it's an absolute bastard of a parameter space to be working in, asymptotically speaking. (That's a technical term from someone who wrote a Ph.D. on panel unit root tests.)

The `ndiffs()` function is from the `forecast` package and suggests a number of differences required to achieve stationarity.

```{r urTest, fig.show = 'asis', include=TRUE}

library(tseries)
adf.test(productivity[,4], alternative = "stationary")

kpss.test(productivity[,4])

ndiffs(productivity[,4])

```

Here the ADF test does not reject the null hypothesis of a unit root, while the KPSS test rejects the null hypothesis of stationarity while `ndiffs()` suggests a single differencing is required. It's all in agreement there.

# Let's try a model with these properties

This isn't my 'modelling process' - truth is, I've tried a bunch of different models on this data and here's what I'm after for it:

    * I want a three-piece trend
    * I'd like to include the labour and capital indices as regressors, but I'll be predicting those as well for the final product
    * There is an ARIMA(1,1,0) here by my best judgement and I'd like regression errors to suit.
    
Here's my basic model without the ARIMA components: not bad, a little overeactive at times.

```{r model1, fig.show = 'asis', include=TRUE}
# Trend terms
t <- time(productivity)
break1 <- 1987
break2 <-2003
t1 <- ts(pmax(0, t - break1), start = 1973)
t2 <- ts(pmax(0, t - break2), start = 1973)

fit.pm <- tslm(productivity[,4] ~ t + t1 + t2 + Capital + LabourHW, data=productivity)

autoplot(productivity[,4]) +
  autolayer(fitted(fit.pm), series = "Piecemeal model") +
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Multifactor Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()


```



If we check the residuals, we can see a whole bunch of autocorrelation simply not been dealt with. We need to deal!

```{r model1Resids, fig.show = 'asis', include=TRUE}
checkresiduals(fit.pm) 
```

Here's what happen if we add a ARIMA(1,1,0) innovations to the mix:

```{r arima, fig.show = 'asis', include=TRUE}
fit <- Arima(productivity[,4], order = c(1,1,0), 
            xreg=cbind(productivity[,c(1,3)], t, t1, t2))

autoplot(productivity[,4]) +
  autolayer(fitted(fit.pm), series = "Piecemeal model") +
  autolayer(fitted(fit), series = "Piecemeal + ARIMA(1,1,0)")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Multifactor Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()

```

This is looking much better- checking residuals: a little skewed maybe, but still significant autocorrelation left in the model.

```{r model2Resids, fig.show = 'asis', include=TRUE}
checkresiduals(fit) 
```

The problem here, I reckon, is the three piece trend I was so fond of - that's in both the regressors: labour and capital so it's probably overfitting. Let's jettison that and use just the regressors and let an integrated AR process mop up the trend stochastically.

```{r arima2, fig.show = 'asis', include=TRUE}
fit <- Arima(productivity[,4], order = c(1,1,0),
            xreg=cbind(productivity[,c(1,3)]))

autoplot(productivity[,4]) +
  autolayer(fitted(fit.pm), series = "Piecemeal model") +
  autolayer(fitted(fit), series = "Auto ARIMA")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Multifactor Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()

checkresiduals(fit)

```

Bingo! No more autocorrelation in the residuals. Let that be a lesson to me forever and ever about over fitting.

# Now we want to forecast multifactor productivity

Now to make a forecast. To do that in this model, we need to forecast capital and labour productivity first. Unsurprisingly, capital and labour productivity have similar time series components to multifactor productivity, so I'll model them both as ARIMA(1,1,0) plus trends. Without the other regressors that multifactor productivity had, we need those trends this time.


```{r labour, fig.show = 'asis', include=TRUE}

fit.labour <- Arima(productivity[,1], order = c(1,1,0), 
                    xreg = cbind(t, t1, t2))

autoplot(productivity[,1]) +
  autolayer(fitted(fit.labour), series = "Piecemeal + ARIMA(1,1,0)")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Labour Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()

checkresiduals(fit.labour)
```

Neither is as good as when you've got regressors to work with, but OK this will do.

```{r capital, fig.show = 'asis', include=TRUE}

fit.capital <- Arima(productivity[,3], order = c(1,1,0), 
                    xreg = cbind(t, t1, t2))

autoplot(productivity[,3]) +
  autolayer(fitted(fit.capital), series = "Piecemeal + ARIMA(1,1,0)")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Capital Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()

checkresiduals(fit.capital)

```

Now, I need to choose a time horizon to forecast capital and labour productivity over. I'm going to go with h = 5 periods, not because I care about periods 2-5, but because it's useful to see how the series is tracking.

```{r fcastlabour, fig.show = 'asis', include=TRUE}
h <- 5
t.new <- t[length(t)]+seq(h)
t1.new <- t1[length(t1)]+seq(h)
t2.new <- t2[length(t2)]+seq(h)
trend.terms <- (cbind(t.new, t1.new, t2.new))

fcast.labour <- forecast(fit.labour, h = 5, xreg = trend.terms)

autoplot(productivity[,1]) +
  autolayer((fcast.labour), PI = TRUE, series = "Forecast")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Labour Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()
```


```{r fcastcapital, fig.show = 'asis', include=TRUE}
fcast.capital <- forecast(fit.capital, h = 5, xreg = trend.terms)

autoplot(productivity[,3]) +
  autolayer((fcast.capital), PI = TRUE, series = "Forecast")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Capital Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()

```

We can use the point forecasts for labour and capital to forecast multifactor productivity.

```{r fcastMFP, fig.show = 'asis', include=TRUE}

l.fcast <- fcast.labour$mean
c.fcast <- fcast.capital$mean
regressorsMFP <- cbind(l.fcast, c.fcast)

fcast.MFP <- forecast(fit, h = 5, xreg = regressorsMFP)

autoplot(productivity[,4]) +
  autolayer((fcast.MFP), PI = TRUE, series = "Forecast")+
  xlab("Year") +  ylab("Productivity Index") +
  ggtitle("Multifactor Productivity") +
  guides(colour=guide_legend(title=" ")) +
  theme_light()


```

So my rough forecast for multifactor productivity (which is admittedly rather overfitted with that piecemeal trend - OK! OK!) is around `r round(fcast.MFP$mean[1],2)`- more or less.

Let's see what the ABS comes up with. But it's also [worthwhile being aware just what a brutally difficult issue this is](https://ftalphaville.ft.com/2017/11/24/2196134/the-obrs-productivity-forecast/).

# References

Australian Bureau of Statistics (2016) Estimates of Industry Multifactor Productivity, Australia (series 5260.0.55.002)](http://www.abs.gov.au/AUSSTATS/abs@.nsf/DetailsPage/5260.0.55.0022015-16?OpenDocument), released on December 5, 2016. Accessed 05/12/16.

Hyndman RJ (2017). _forecast: Forecasting functions for time series and
linear models_. R package version 8.2, <URL:
http://pkg.robjhyndman.com/forecast>.

Hyndman RJ and Khandakar Y (2008). “Automatic time series forecasting:
the forecast package for R.” _Journal of Statistical Software_, *26*(3),
pp. 1-22. <URL: http://www.jstatsoft.org/article/view/v027i03>.

D. Kwiatkowski, P. C. B. Phillips, P. Schmidt, and Y. Shin (1992): Testing the Null Hypothesis of Stationarity against the Alternative of a Unit Root. Journal of Econometrics 54, 159–178.

S. E. Said and D. A. Dickey (1984): Testing for Unit Roots in Autoregressive-Moving Average Models of Unknown Order. Biometrika 71, 599–607.

Adrian Trapletti and Kurt Hornik (2017). tseries: Time Series Analysis
  and Computational Finance. R package version 0.10-42.
  
Hadley Wickham (2017). tidyverse: Easily Install and Load 'Tidyverse'
  Packages. R package version 1.1.1.
  https://CRAN.R-project.org/package=tidyverse



